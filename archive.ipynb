{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitparse\n",
    "from plotnine import *\n",
    "from functions import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from scipy import signal\n",
    "\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import math\n",
    "\n",
    "\n",
    "import os\n",
    "os.chdir('C:/Users/Olsen/Desktop/Masteroppgave/Data/workfileexport - decompressed/')\n",
    "\n",
    "fitfile = fitparse.FitFile('tp-3912799.2023-07-01-09-39-18-438Z.GarminPing.AAAAAGSf9EY7-gSP.FIT')\n",
    "\n",
    "\n",
    "HRV_data = get_fit_hrv(fitfile)\n",
    "HR_data = get_fit_hr(fitfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check RR from Polar (data from Project thesis for vertification of filterings)\n",
    "def RR_data_txt(file_txt):\n",
    "    # Extract RR and timestamps from txt file\n",
    "    RRs_polar = []\n",
    "    timestamps_polar = []\n",
    "\n",
    "    f_file = open(file_txt,'r')\n",
    "    next(f_file)\n",
    "    # input format\n",
    "    format = '%Y-%m-%dT%H:%M:%S.%f'\n",
    "\n",
    "    for row in f_file:\n",
    "        row = row.split(';')\n",
    "        timestamps_polar.append(datetime.strptime(row[0], format))\n",
    "        RRs_polar.append(int(row[1]))\n",
    "    result = [timestamps_polar, RRs_polar]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact_correction_threshold = 0.05\n",
    "# filtered_RRs = []\n",
    "\n",
    "# for i in range(len(HRV_data)):\n",
    "#   if HRV_data[(i-1)]*(1-artifact_correction_threshold) < HRV_data[i] < HRV_data[(i-1)]*(1+artifact_correction_threshold):\n",
    "#         filtered_RRs.append(HRV_data[i])\n",
    "\n",
    "# x = np.cumsum(filtered_RRs)\n",
    "\n",
    "\n",
    "HRV_filtered = scipy.signal.medfilt(HRV_data, 3)\n",
    "x = np.cumsum(HRV_filtered)\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['timestamp'] = x\n",
    "df['RR'] = HRV_filtered\n",
    "\n",
    "# ----------------------------------\n",
    "#  Plotting\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df[\"timestamp\"],\n",
    "    y=df[\"RR\"]*1000,\n",
    "    name=\"HRV\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"HRV\",\n",
    "    xaxis_title=\"Minuttes\", yaxis_title=\"HRV [ms]\",\n",
    "    legend_title=\"Legend\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\", size=18, color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered(input_signal):\n",
    "    filtered_signal = []\n",
    "    for i in range(len(input_signal)):\n",
    "        if input_signal[(i-1)]*0.5 < input_signal[i] or input_signal[i] < input_signal[(i-1)]*2:\n",
    "            filtered_signal.append(input_signal[i])\n",
    "    return  np.array(filtered_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtered_signal(input_signal):\n",
    "    filtered_signal = []\n",
    "    for i in range(len(input_signal)):\n",
    "        if input_signal[(i-1)]*0.5 < input_signal[i] < input_signal[(i-1)]*1.25:\n",
    "            filtered_signal.append(input_signal[i])\n",
    "        else:\n",
    "            filtered_signal.append(input_signal[i]/2)\n",
    "            filtered_signal.append(input_signal[i]/2)\n",
    "    return  filtered_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_artifacts(data, threshold):\n",
    "    interpolated_data = data.copy()  # Create a copy of the original data\n",
    "\n",
    "    for i in range(1, len(data) - 1):\n",
    "        if abs(data[i] - data[i - 1]) > threshold and abs(data[i] - data[i + 1]) > threshold:\n",
    "            # If the difference between the current point and its neighbors exceeds the threshold\n",
    "            interpolated_data[i] = (data[i - 1] + data[i + 1]) / 2  # Interpolate the value\n",
    "\n",
    "    return interpolated_data\n",
    "\n",
    "\n",
    "def interpolate_missing_packages(data):\n",
    "    interpolated_data = data.copy()\n",
    "\n",
    "    for i in range(2, len(data)):\n",
    "        # Assuming the threshold to detect anomalies\n",
    "        threshold = 1.25 * data[i - 1]\n",
    "\n",
    "        if data[i] > threshold:\n",
    "            # Interpolate missing values based on different scenarios of package loss\n",
    "            if data[i] > 1.75 * data[i - 1]:\n",
    "                interpolated_data[i] = (data[i - 1] + data[i - 2]) / 2\n",
    "            elif data[i] > 2.75 * data[i - 1]:\n",
    "                interpolated_data[i] = (data[i - 1] + data[i - 2] + data[i - 3]) / 3\n",
    "        elif data[i] > threshold:\n",
    "            interpolated_data[i] = 0\n",
    "    return interpolated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFA computation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DFA(pp_values, lower_scale_limit, upper_scale_limit):\n",
    "    scaleDensity = 30 # scales DFA is conducted between lower_scale_limit and upper_scale_limit\n",
    "    m = 1 # order of polynomial fit (linear = 1, quadratic m = 2, cubic m = 3, etc...)\n",
    "\n",
    "    # initialize, we use logarithmic scales\n",
    "    start = np.log(lower_scale_limit) / np.log(10)\n",
    "    stop = np.log(upper_scale_limit) / np.log(10)\n",
    "    scales = np.floor(np.logspace(np.log10(math.pow(10, start)), np.log10(math.pow(10, stop)), scaleDensity))\n",
    "    F = np.zeros(len(scales))\n",
    "    count = 0\n",
    "\n",
    "    for s in scales:\n",
    "        rms = []\n",
    "        # Step 1: Determine the \"profile\" (integrated signal with subtracted offset)\n",
    "        x = pp_values\n",
    "        y_n = np.cumsum(x - np.mean(x))\n",
    "        # Step 2: Divide the profile into N non-overlapping segments of equal length s\n",
    "        L = len(x)\n",
    "        shape = [int(s), int(np.floor(L/s))]\n",
    "        nwSize = int(shape[0]) * int(shape[1])\n",
    "        # beginning to end, here we reshape so that we have a number of segments based on the scale used at this cycle\n",
    "        Y_n1 = np.reshape(y_n[0:nwSize], shape, order=\"F\")\n",
    "        Y_n1 = Y_n1.T\n",
    "        # end to beginning\n",
    "        Y_n2 = np.reshape(y_n[len(y_n) - (nwSize):len(y_n)], shape, order=\"F\")\n",
    "        Y_n2 = Y_n2.T\n",
    "        # concatenate\n",
    "        Y_n = np.vstack((Y_n1, Y_n2))\n",
    "\n",
    "        # Step 3: Calculate the local trend for each 2Ns segments by a least squares fit of the series\n",
    "        for cut in np.arange(0, 2 * shape[1]):\n",
    "            xcut = np.arange(0, shape[0])\n",
    "            pl = np.polyfit(xcut, Y_n[cut,:], m)\n",
    "            Yfit = np.polyval(pl, xcut)\n",
    "            arr = Yfit - Y_n[cut,:]\n",
    "            rms.append(np.sqrt(np.mean(arr * arr)))\n",
    "\n",
    "        if (len(rms) > 0):\n",
    "            F[count] = np.power((1 / (shape[1] * 2)) * np.sum(np.power(rms, 2)), 1/2)\n",
    "        count = count + 1\n",
    "\n",
    "    pl2 = np.polyfit(np.log2(scales), np.log2(F), 1)\n",
    "    alpha = pl2[0]\n",
    "    return alpha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def computeFeatures(df):\n",
    "  features = []\n",
    "  step = 120\n",
    "  for index in range(0, int(round(np.max(x)/step))):\n",
    "      \n",
    "      array_rr = df.loc[(df['timestamp'] >= (index*step)) & (df['timestamp'] <= (index+1)*step), 'RR']*1000\n",
    "      # compute heart rate\n",
    "      heartrate = round(60000/np.mean(array_rr), 2)\n",
    "      # compute rmssd\n",
    "      NNdiff = np.abs(np.diff(array_rr))\n",
    "      rmssd = round(np.sqrt(np.sum((NNdiff * NNdiff) / len(NNdiff))), 2)\n",
    "      # compute sdnn \n",
    "      sdnn = round(np.std(array_rr), 2)\n",
    "      #dfa, alpha 1\n",
    "      alpha1 = DFA(array_rr.to_list(), 4, 16)\n",
    "\n",
    "      curr_features = {\n",
    "          'timestamp': index*step,\n",
    "          'heartrate': heartrate,\n",
    "          'rmssd': rmssd,\n",
    "          'sdnn': sdnn,\n",
    "          'alpha1': alpha1,\n",
    "      }\n",
    "\n",
    "      features.append(curr_features)\n",
    "\n",
    "  features_df = pd.DataFrame(features)\n",
    "  return features_df    \n",
    "\n",
    "\n",
    "features_df = computeFeatures(df)\n",
    "features_df.head()\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot the synthetic signal on the primary y-axis\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Signal', color='tab:blue')\n",
    "ax1.plot(df[\"timestamp\"], 60/df[\"RR\"], color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Plot DFA Alpha 1 on the secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('DFA Alpha 1', color='tab:red')\n",
    "ax2.plot(features_df[\"timestamp\"], features_df[\"alpha1\"], \"-o\", color='tab:red', label='DFA Alpha 1')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
    "ax2.fill_between(x, 0.75, 0.5, color='C0', alpha=0.2)\n",
    "ax2.fill_between(x, 0.5, 0, color='C0', alpha=0.1)\n",
    "\n",
    "plt.title('HR and DFA Alpha 1 over Time')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(features_df['alpha1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def DFA(pp_values, lower_scale_limit, upper_scale_limit):\n",
    "#     scaleDensity = 30 # scales DFA is conducted between lower_scale_limit and upper_scale_limit\n",
    "#     m = 1 # order of polynomial fit (linear = 1, quadratic m = 2, cubic m = 3, etc...)\n",
    "\n",
    "#     initialize, we use logarithmic scales\n",
    "#     start = np.log(lower_scale_limit) / np.log(10)\n",
    "#     stop = np.log(upper_scale_limit) / np.log(10)\n",
    "#     scales = np.floor(np.logspace(np.log10(math.pow(10, start)), np.log10(math.pow(10, stop)), scaleDensity))\n",
    "#     F = np.zeros(len(scales))\n",
    "#     count = 0\n",
    "\n",
    "#     for s in scales:\n",
    "#         rms = []\n",
    "#         Step 1: Determine the \"profile\" (integrated signal with subtracted offset)\n",
    "#         x = pp_values\n",
    "#         y_n = np.cumsum(x - np.mean(x))\n",
    "#         Step 2: Divide the profile into N non-overlapping segments of equal length s\n",
    "#         L = len(x)\n",
    "#         shape = [int(s), int(np.floor(L/s))]\n",
    "#         nwSize = int(shape[0]) * int(shape[1])\n",
    "#         beginning to end, here we reshape so that we have a number of segments based on the scale used at this cycle\n",
    "#         Y_n1 = np.reshape(y_n[0:nwSize], shape, order=\"F\")\n",
    "#         Y_n1 = Y_n1.T\n",
    "#         end to beginning\n",
    "#         Y_n2 = np.reshape(y_n[len(y_n) - (nwSize):len(y_n)], shape, order=\"F\")\n",
    "#         Y_n2 = Y_n2.T\n",
    "#         concatenate\n",
    "#         Y_n = np.vstack((Y_n1, Y_n2))\n",
    "\n",
    "#         Step 3: Calculate the local trend for each 2Ns segments by a least squares fit of the series\n",
    "#         for cut in np.arange(0, 2 * shape[1]):\n",
    "#             xcut = np.arange(0, shape[0])\n",
    "#             pl = np.polyfit(xcut, Y_n[cut,:], m)\n",
    "#             Yfit = np.polyval(pl, xcut)\n",
    "#             arr = Yfit - Y_n[cut,:]\n",
    "#             rms.append(np.sqrt(np.mean(arr * arr)))\n",
    "\n",
    "#         if (len(rms) > 0):\n",
    "#             F[count] = np.power((1 / (shape[1] * 2)) * np.sum(np.power(rms, 2)), 1/2)\n",
    "#         count = count + 1\n",
    "\n",
    "#     pl2 = np.polyfit(np.log2(scales), np.log2(F), 1)\n",
    "#     alpha = pl2[0]\n",
    "#     return alpha\n",
    "\n",
    "\n",
    "# def computeFeatures(df):\n",
    "#   features = []\n",
    "#   step = 120\n",
    "#   x = np.multiply(df[\"RR\"][0:step], 1000)\n",
    "\n",
    "#   for index in range(0,int(round(np.max(x)/step))):\n",
    "#       array_rr = df.loc[(df['timestamp'] >= (index*step)) & (df['timestamp'] <= (index+1)*step), 'RR'*1000]\n",
    "#       compute heart rate\n",
    "#       heartrate = round(60000/np.mean(array_rr), 2)\n",
    "#       compute rmssd\n",
    "#       NNdiff = np.abs(np.diff(array_rr))\n",
    "#       rmssd = round(np.sqrt(np.sum((NNdiff * NNdiff) / len(NNdiff))), 2)\n",
    "#       compute sdnn \n",
    "#       sdnn = round(np.std(array_rr), 2)\n",
    "#       dfa, alpha 1\n",
    "#       alpha1 = DFA(array_rr.to_list(), 4, 16)\n",
    "\n",
    "#       curr_features = {\n",
    "#           'timestamp': index*step,\n",
    "#           'heartrate': heartrate,\n",
    "#           'rmssd': rmssd,\n",
    "#           'sdnn': sdnn,\n",
    "#           'alpha1': alpha1,\n",
    "#       }\n",
    "\n",
    "#       features.append(curr_features)\n",
    "\n",
    "#   features_df = pd.DataFrame(features)\n",
    "#   return features_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfa_continous(data, window_size):\n",
    "    n = len(data)\n",
    "    scales = np.logspace(1, np.log10(window_size), num=20, dtype=int)\n",
    "    fluctuations = []\n",
    "\n",
    "    for i in range(0, n - window_size + 1):\n",
    "        current_window = data[i:i + window_size]\n",
    "        window_n = len(current_window)\n",
    "\n",
    "        scale_fluctuations = []\n",
    "\n",
    "        for scale in scales:\n",
    "            num_segments = window_n // scale\n",
    "            fluctuation = 0.0\n",
    "\n",
    "            for segment in range(num_segments):\n",
    "                start = segment * scale\n",
    "                end = (segment + 1) * scale\n",
    "                segment_data = current_window[start:end]\n",
    "                segment_fit = np.polyfit(np.arange(start, end), segment_data, 1)\n",
    "                segment_trend = np.polyval(segment_fit, np.arange(start, end))\n",
    "                segment_fluctuation = np.sqrt(np.mean((segment_data - segment_trend) ** 2))\n",
    "                fluctuation += segment_fluctuation\n",
    "\n",
    "            fluctuation /= num_segments\n",
    "            scale_fluctuations.append(fluctuation)\n",
    "\n",
    "        alpha, _ = np.polyfit(np.log10(scales), np.log10(scale_fluctuations), 1)\n",
    "        fluctuations.append(abs(alpha))\n",
    "    return fluctuations\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
